{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries, paths, and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import CatBoostEncoder\n",
    "import src.utils.utils\n",
    "import src.data.window\n",
    "\n",
    "# Changing directory\n",
    "os.chdir('/Users/manotas/Documents/GitHub-Repos/ML-Energy-Colombia')\n",
    "\n",
    "# Setting storage paths\n",
    "storage_path = 'data/processed/'\n",
    "\n",
    "# Loading the full data\n",
    "fulldata = pd.read_csv(os.path.join(storing_path, 'fulldata.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying all object columns except 'plant' and 'agent'\n",
    "object_columns = [col for col in fulldata.columns if fulldata[col].dtype == 'object' and col not in ['plant', 'agent']]\n",
    "\n",
    "# Identifying all non-object columns\n",
    "non_object_columns = [col for col in fulldata.columns if fulldata[col].dtype != 'object']\n",
    "\n",
    "# Rearranging fulldata so object columns (except 'plant' and 'agent') come last, followed by 'plant' and 'agent'\n",
    "fulldata = fulldata[non_object_columns + object_columns + ['plant', 'agent']]\n",
    "\n",
    "# Creating a mask based on the datetime condition\n",
    "mask = fulldata['datetime'] < '2022-01-01'\n",
    "\n",
    "# Splitting features and target variable\n",
    "X = fulldata.drop(['daily_ask'], axis=1)  # Drop the target variable to separate features\n",
    "Y = fulldata['daily_ask']  # Target variable\n",
    "\n",
    "# Applying the mask to split the features\n",
    "X_train = X[mask]\n",
    "X_test = X[~mask]\n",
    "\n",
    "# Applying the mask to split the target variable\n",
    "Y_train = Y[mask]\n",
    "Y_test = Y[~mask]\n",
    "\n",
    "# Saving datetime to index afterwards\n",
    "train_dt = X_train['datetime'].copy()\n",
    "test_dt = X_test['datetime'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically identifying categorical and numerical columns, excluding the target variable 'daily_ask'\n",
    "categorical_columns = [col for col in X_train.columns if X_train[col].dtype == 'object' and col not in ['plant','agent']]\n",
    "numerical_columns = [col for col in X_train.columns if X_train[col].dtype != 'object' and col not in ['datetime']]  # Assuming 'datetime' needs special handling or is excluded\n",
    "\n",
    "# Combining preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyclic', CyclicalDateTimeFeatures(), ['datetime']),\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', CatBoostEncoder(), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough')\n",
    "\n",
    "# Creating the preprocessing pipeline\n",
    "encoding_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Now fitting this pipeline to the training data and transform both sets\n",
    "encoding_pipeline.fit(X_train, Y_train)  # Fit to the training data\n",
    "X_train_transformed = encoding_pipeline.transform(X_train)  # Transform training data\n",
    "X_test_transformed = encoding_pipeline.transform(X_test)  # Transform testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the cyclical features generated by CyclicalDateTimeFeatures transformer\n",
    "cyclical_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "# Original order of columns, replacing 'datetime' with the cyclical features, \n",
    "# and ensuring 'plant' and 'agent' are correctly positioned\n",
    "new_order = cyclical_features + [col for col in numerical_columns + categorical_columns if col not in ['plant', 'agent']] + ['plant'] + ['agent']\n",
    "\n",
    "# Converting transformed arrays back into DataFrames with the new column order\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=new_order, index=X_train.index)\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=new_order, index=X_test.index)\n",
    "\n",
    "# Reapplying the datetime information as the index\n",
    "X_train_transformed_df.index = pd.to_datetime(train_dt)\n",
    "X_test_transformed_df.index = pd.to_datetime(test_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving transformed DataFrames for ease of access\n",
    "X_train = X_train_transformed_df.copy()\n",
    "X_train['datetime'] = X_train.index\n",
    "X_train.set_index(['datetime', 'plant'], inplace=True)\n",
    "X_train.to_csv(os.path.join(storing_path, 'X_train.csv'))\n",
    "\n",
    "X_test = X_test_transformed_df.copy()\n",
    "X_test['datetime'] = X_test.index\n",
    "X_test.set_index(['datetime', 'plant'], inplace=True)\n",
    "X_test.to_csv(os.path.join(storing_path, 'X_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading saved DataFrames\n",
    "X_train = pd.read_csv(os.path.join(storing_path, 'X_train.csv'))\n",
    "X_test = pd.read_csv(os.path.join(storing_path, 'X_test.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining window data\n",
    "We define a formula to build specific-sized windows of data for plants and datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing plants: 100%|██████████| 70/70 [16:52<00:00, 14.46s/it]\n",
      "Processing plants: 100%|██████████| 70/70 [05:55<00:00,  5.08s/it]\n"
     ]
    }
   ],
   "source": [
    "window_size = 24  # for example, a window size of 24 hours\n",
    "n_ovr_windows = create_windows(X_train, window_size, overlap=False)\n",
    "ovr_windows = create_windows(X_train, window_size, overlap=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving window data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the non-overlapping windows list\n",
    "with open(os.path.join(storing_path, 'non_overlapping_windows.pkl'), 'wb') as f:\n",
    "    pickle.dump(n_ovr_windows, f)\n",
    "\n",
    "# Saving the overlapping windows list\n",
    "with open(os.path.join(storing_path, 'overlapping_windows.pkl'), 'wb') as f:\n",
    "    pickle.dump(ovr_windows, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
