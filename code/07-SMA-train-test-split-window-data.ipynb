{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries, paths, and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import CatBoostEncoder\n",
    "\n",
    "# Changing directory\n",
    "os.chdir('/Users/manotas/Documents/GitHub-Repos/ML-Energy-Colombia')\n",
    "\n",
    "# Local libraries\n",
    "from src.utils.utils import CyclicalDateTimeFeatures, datetimer\n",
    "from src.data.window import create_windows, split_features_targets\n",
    "\n",
    "\n",
    "# Setting storage paths\n",
    "storage_path = 'data/processed/'\n",
    "\n",
    "# Loading the full data\n",
    "try:\n",
    "    fulldata = pd.read_csv(os.path.join(storage_path, 'fulldata.csv'))\n",
    "except FileNotFoundError:\n",
    "    print(\"The file does not exist\")\n",
    "\n",
    "fulldata = datetimer(fulldata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying all object columns except 'plant' and 'agent'\n",
    "object_columns = [col for col in fulldata.columns if fulldata[col].dtype == 'object' and col not in ['plant', 'agent']]\n",
    "\n",
    "# Identifying all non-object columns\n",
    "non_object_columns = [col for col in fulldata.columns if fulldata[col].dtype != 'object']\n",
    "\n",
    "# Rearranging fulldata so object columns (except 'plant' and 'agent') come last, followed by 'plant' and 'agent'\n",
    "fulldata = fulldata[non_object_columns + object_columns + ['plant', 'agent']]\n",
    "\n",
    "# Creating a mask based on the datetime condition\n",
    "mask = fulldata['datetime'] < '2022-01-01'\n",
    "\n",
    "# Splitting features and target variable\n",
    "X = fulldata.drop(['daily_ask'], axis=1)  # Drop the target variable to separate features\n",
    "Y = fulldata[['daily_ask']]  # Target variable (kept as a DataFrame to facilitate windowing)\n",
    "\n",
    "# Creating a mask based on the datetime condition\n",
    "mask = fulldata['datetime'] < '2022-01-01'\n",
    "\n",
    "# Applying the mask to split the features and target variable\n",
    "X_train = X[mask]\n",
    "X_test = X[~mask]\n",
    "Y_train = Y[mask]\n",
    "Y_test = Y[~mask]\n",
    "\n",
    "# Saving datetime to index afterwards\n",
    "train_dt = X_train['datetime'].copy()\n",
    "test_dt = X_test['datetime'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically identifying categorical and numerical columns, excluding the target variable 'daily_ask'\n",
    "categorical_columns = [col for col in X_train.columns if X_train[col].dtype == 'object' and col not in ['plant','agent']]\n",
    "numerical_columns = [col for col in X_train.columns if X_train[col].dtype != 'object' and col not in ['datetime']]  # Assuming 'datetime' needs special handling or is excluded\n",
    "\n",
    "# Combining preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyclic', CyclicalDateTimeFeatures(), ['datetime']),\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', CatBoostEncoder(), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough')\n",
    "\n",
    "# Creating the preprocessing pipeline\n",
    "encoding_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Now fitting this pipeline to the training data and transform both sets\n",
    "encoding_pipeline.fit(X_train, Y_train)  # Fit to the training data\n",
    "X_train_transformed = encoding_pipeline.transform(X_train)  # Transform training data\n",
    "X_test_transformed = encoding_pipeline.transform(X_test)  # Transform testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the cyclical features generated by CyclicalDateTimeFeatures transformer\n",
    "cyclical_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "# Original order of columns, replacing 'datetime' with the cyclical features, \n",
    "# and ensuring 'plant' and 'agent' are correctly positioned\n",
    "new_order = cyclical_features + [col for col in numerical_columns + categorical_columns if col not in ['plant', 'agent']] + ['plant'] + ['agent']\n",
    "\n",
    "# Converting transformed arrays back into DataFrames with the new column order\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=new_order, index=X_train.index)\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=new_order, index=X_test.index)\n",
    "\n",
    "# Reapplying the datetime information as the index\n",
    "X_train_transformed_df.index = pd.to_datetime(train_dt)\n",
    "X_test_transformed_df.index = pd.to_datetime(test_dt)\n",
    "\n",
    "# Ensuring indices are unique\n",
    "X_train_transformed_df = X_train_transformed_df.loc[~X_train_transformed_df.index.duplicated(keep='first')]\n",
    "X_test_transformed_df = X_test_transformed_df.loc[~X_test_transformed_df.index.duplicated(keep='first')]\n",
    "Y_train = Y_train.loc[~Y_train.index.duplicated(keep='first')]\n",
    "Y_test = Y_test.loc[~Y_test.index.duplicated(keep='first')]\n",
    "\n",
    "# Combining `X_train` and `Y_train` for windowing\n",
    "X_train_transformed_df['datetime'] = train_dt  # Adding datetime column back\n",
    "X_test_transformed_df['datetime'] = test_dt    # Adding datetime column back\n",
    "\n",
    "train_data = pd.concat([X_train_transformed_df, Y_train], axis=1)\n",
    "test_data = pd.concat([X_test_transformed_df, Y_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining window data\n",
    "We define a formula to build specific-sized windows of data for plants and datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing plants: 100%|██████████| 70/70 [00:23<00:00,  2.95it/s]\n",
      "Processing plants: 100%|██████████| 69/69 [00:02<00:00, 32.72it/s]\n",
      "Processing plants: 100%|██████████| 70/70 [00:29<00:00,  2.37it/s]\n",
      "Processing plants: 100%|██████████| 69/69 [00:03<00:00, 18.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create windows for training and test sets\n",
    "window_size = 24  # Assuming 24 hours\n",
    "\n",
    "train_windows_n = create_windows(train_data, window_size, overlap=False)\n",
    "test_windows_n = create_windows(test_data, window_size, overlap=False)\n",
    "\n",
    "train_windows_o = create_windows(train_data, window_size, overlap=False)\n",
    "test_windows_o = create_windows(test_data, window_size, overlap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the splitting function\n",
    "X_train_windows_n, Y_train_windows_n = split_features_targets(train_windows_n, 'daily_ask')\n",
    "X_test_windows_n, Y_test_windows_n = split_features_targets(test_windows_n, 'daily_ask')\n",
    "\n",
    "# Applying the splitting function\n",
    "X_train_windows_o, Y_train_windows_o = split_features_targets(train_windows_o, 'daily_ask')\n",
    "X_test_windows_o, Y_test_windows_o = split_features_targets(test_windows_o, 'daily_ask')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving window data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the non-overlapping windows list\n",
    "with open(os.path.join(storage_path, 'train_non_overlapping_windows.pkl'), 'wb') as f:\n",
    "    pickle.dump((X_train_windows_n, Y_train_windows_n), f)\n",
    "\n",
    "with open(os.path.join(storage_path, 'test_non_overlapping_windows.pkl'), 'wb') as f:\n",
    "    pickle.dump((X_test_windows_n, Y_test_windows_n), f)\n",
    "\n",
    "with open(os.path.join(storage_path, 'train_overlapping_windows.pkl'), 'wb') as f:\n",
    "    pickle.dump((X_train_windows_o, Y_train_windows_o), f)\n",
    "\n",
    "with open(os.path.join(storage_path, 'test_overlapping_windows.pkl'), 'wb') as f:\n",
    "    pickle.dump((X_test_windows_o, Y_test_windows_o), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
